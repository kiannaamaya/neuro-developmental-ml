{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vmybzLUiNcNf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from openpyxl import load_workbook\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BT3q7KpODlW"
   },
   "source": [
    "Following this code, https://github.com/pacocp/multiomic-fusion-NSCLC/blob/master/src/integration_late_fusion_all.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMhrxSXWPr_Y",
    "outputId": "d4576971-c348-49ab-a15d-b7aac2b37b23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x127b48a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eOislJHTNjlx"
   },
   "outputs": [],
   "source": [
    "class model_together(nn.Module):\n",
    "    def __init__(self, num_dtypes):\n",
    "        super(model_together, self).__init__()\n",
    "        self.weights = nn.Linear(num_dtypes, 3)\n",
    "        #self.fc = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self,xb):\n",
    "        out = self.weights(xb)\n",
    "        #out = self.fc(F.relu(out))\n",
    "        out = torch.diagonal(out, 0, dim1=-2, dim2=-1)\n",
    "        #out = F.softmax(out, dim=1)\n",
    "\n",
    "        return out\n",
    "def init_params(size, std=1.0): return (torch.rand(size)*std).requires_grad_()\n",
    "def init_model(model, num_dtypes):\n",
    "    with torch.no_grad():\n",
    "        params = init_params((3,num_dtypes))\n",
    "        params = params / params.sum(1, keepdim=True)\n",
    "        model.weights.weight.copy_(params).float()\n",
    "        model.weights.bias.fill_(0)\n",
    "\n",
    "def norm_params(model):\n",
    "    with torch.no_grad():\n",
    "        # Push positive before scaling:\n",
    "        #model.model.weight[model.model.weight < 0] = 0.\n",
    "        #weights_norm = model.model.weight / model.model.weight.sum(1, keepdim=True)\n",
    "        weights_norm = F.softmax(model.weights.weight, dim=1)\n",
    "        model.weights.weight.copy_(weights_norm).float()\n",
    "        model.weights.bias.fill_(0)\n",
    "\n",
    "# this is ok to be with numpy but then need to be converted to tensors\n",
    "def get_val_set(x, y, classes, percentage = 0.1):\n",
    "    np.random.seed(42)\n",
    "    x_train = np.array([]).reshape(0,x.shape[1])\n",
    "    y_train = np.array([]).reshape(0,y.shape[1])\n",
    "    x_val = np.array([]).reshape(0,x.shape[1])\n",
    "    y_val = np.array([]).reshape(0,y.shape[1])\n",
    "    for c in classes:\n",
    "        indexes = np.where(y.argmax(axis=1) == c)[0]\n",
    "        np.random.shuffle(indexes)\n",
    "        len_val = int(percentage * len(indexes))\n",
    "        len_train = len(indexes) - len_val\n",
    "        index_train = indexes[0:len_train]\n",
    "        index_val = indexes[len_train:]\n",
    "        x_train = np.concatenate([x_train, x[index_train,...]], axis=0)\n",
    "        y_train = np.concatenate([y_train, y[index_train]], axis=0)\n",
    "        x_val = np.concatenate([x_val, x[index_val,...]], axis=0)\n",
    "        y_val = np.concatenate([y_val, y[index_val]], axis=0)\n",
    "\n",
    "    index_train = list(range(x_train.shape[0]))\n",
    "    index_val = list(range(x_val.shape[0]))\n",
    "    np.random.shuffle(index_train)\n",
    "    np.random.shuffle(index_val)\n",
    "\n",
    "    return x_train[index_train,...],y_train[index_train], x_val[index_val,...], y_val[index_val]\n",
    "\n",
    "def train_model(model, dls, optimizer, criterion, num_epochs):\n",
    "# copy code from the other notebook\n",
    "    # Start training\n",
    "    best_epoch = 0\n",
    "    best_acc = 100\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_labels_auc = []\n",
    "    val_preds_auc = []\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    train_preds_auc = []\n",
    "    train_labels_auc = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    losses = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "    sizes = {'train': 0, 'val': 0}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        batch_loss = 0\n",
    "        sizes = {}\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            sizes[phase] = 0\n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dls[phase]:\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                #labels = F.one_hot(labels, num_classes=3)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    #import pdb; pdb.set_trace()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                if phase == 'val':\n",
    "                    val_preds += list(preds.numpy())\n",
    "                    val_labels += list(labels.numpy())\n",
    "                    val_labels_auc += [labels.numpy()]\n",
    "                    val_preds_auc += [preds.numpy()]\n",
    "\n",
    "\n",
    "                # Accumulating the loss over time\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels)\n",
    "                sizes[phase] += inputs.size(0)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    train_preds += list(preds.numpy())\n",
    "                    train_labels += list(labels.numpy())\n",
    "                    train_labels_auc += [labels.numpy()]\n",
    "                    train_preds_auc += [preds.numpy()]\n",
    "                    # Getting gradients w.r.t. parameters\n",
    "                    loss.backward()\n",
    "                    # Updating parameters\n",
    "                    optimizer.step()\n",
    "                    #norm_params(model)\n",
    "\n",
    "            epoch_loss = running_loss / sizes[phase]\n",
    "            epoch_acc = running_corrects.item() / sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "            losses[phase].append(epoch_loss)\n",
    "            if phase == 'val' and epoch_loss < best_acc:\n",
    "                best_epoch = epoch\n",
    "                best_acc = epoch_loss\n",
    "                #best_auc = epoch_auc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Lr scheduler\n",
    "        #lr_scheduler.step()\n",
    "\n",
    "    print('Best epoch {}'.format(best_epoch))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    '''\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(num_epochs)),losses['train'], label='train', color='blue')\n",
    "    plt.plot(list(range(num_epochs)),losses['val'], label='val', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    '''\n",
    "    norm_params(model)\n",
    "    return model\n",
    "\n",
    "def get_alphas_SGD(df, classes, data_types):\n",
    "    x_train = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        x = [[],[],[]]\n",
    "        for d_type in data_types:\n",
    "            if row['Has ' + d_type] != -1:\n",
    "                luad_new = row[d_type+ ' Prob LUAD']\n",
    "                hlt_new = row[d_type+ ' Prob HLT']\n",
    "                lusc_new = row[d_type+ ' Prob LUSC']\n",
    "                #luad_new = luad / (luad + hlt +lusc)\n",
    "                #hlt_new = hlt / (luad + hlt +lusc)\n",
    "                #lusc_new = lusc / (luad + hlt +lusc)\n",
    "                x[0].append(luad_new)\n",
    "                x[1].append(hlt_new)\n",
    "                x[2].append(lusc_new)\n",
    "            else:\n",
    "                x[0].append(0)\n",
    "                x[1].append(0)\n",
    "                x[2].append(0)\n",
    "        x_train.append(x)\n",
    "    x_train = np.asarray(x_train)\n",
    "    real = df['Real'].values\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
    "\n",
    "    for train_index, test_index  in sss.split(x_train, real):\n",
    "        train_x, train_y = x_train[train_index,:,:], real[train_index]\n",
    "        val_x, val_y = x_train[test_index,:,:], real[test_index]\n",
    "\n",
    "    dl_train = DataLoader(list(zip(train_x,train_y)), batch_size=32, shuffle=True)\n",
    "    dl_val = DataLoader(list(zip(val_x,val_y)), batch_size=32, shuffle=False)\n",
    "    dls = {'train': dl_train, 'val': dl_val}\n",
    "\n",
    "    model = model_together(len(data_types))\n",
    "    init_model(model, len(data_types))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 0.01)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = train_model(model, dls, optimizer, loss, num_epochs=5)\n",
    "\n",
    "    # a\n",
    "    # once the optimization has been carried out\n",
    "    weights = model.weights.weight.detach().numpy()\n",
    "\n",
    "    alphas = {}\n",
    "    index_dtype = 0\n",
    "    for d_type in data_types:\n",
    "        alphas[d_type] = []\n",
    "        for i in range(len(classes)):\n",
    "            alphas[d_type].append(weights[i,index_dtype])\n",
    "        index_dtype += 1\n",
    "    return alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "-nwvAJSoOOhF"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def integration_model(data_types: List[str], datasets: List[str], name: str, path1: str, path2: str,\n",
    "                      fusion_type='probs', use_alphas=False, m_alphas=None) -> None:\n",
    "\n",
    "    for d in datasets:\n",
    "        csv_filename = path1 + 'data_integration_model_' + d + '_' + fusion_type + '_' + name + '.csv'\n",
    "\n",
    "        # Data is read directly from CSV files\n",
    "        data = {}\n",
    "        for dt in data_types:\n",
    "            file_name = f\"{dt}_1.csv\"\n",
    "            data[dt] = pd.read_csv(path1 + dt + '/' + file_name)\n",
    "\n",
    "        if use_alphas:\n",
    "            print('Getting alphas from training set...')\n",
    "            data_train = {}\n",
    "            for dt in data_types:\n",
    "                # Loading X_train and y_train separately\n",
    "                with open(path2 + f'{dt}' + '/X_train.pkl', 'rb') as f:\n",
    "                    X_train = joblib.load(f)\n",
    "                with open(path2 + f'{dt}' + '/y_train.pkl', 'rb') as f:\n",
    "                    y_train = joblib.load(f)\n",
    "\n",
    "                print(\"X_train Length: \", len(X_train))\n",
    "                print(\"y_train Length: \", len(y_train))\n",
    "\n",
    "                # Ensure X_train is a DataFrame\n",
    "                if not isinstance(X_train, pd.DataFrame):\n",
    "                    X_train = pd.DataFrame(X_train, columns=[f'feature_{i}' for i in range(X_train.shape[1])])\n",
    "\n",
    "                # Ensure y_train is a Series\n",
    "                if not isinstance(y_train, pd.Series):\n",
    "                    y_train = pd.Series(y_train, name='age_group')\n",
    "\n",
    "                # Add y_train as a new column to X_train\n",
    "                X_train['age_group'] = y_train.values\n",
    "                data_train[dt] = X_train\n",
    "            \n",
    "            splits_alphas = {}\n",
    "            for dt, df in data_train.items():\n",
    "                alphas = get_alphas_SGD(df, [0,1,2], data_types)\n",
    "                splits_alphas[dt] = alphas\n",
    "                print(alphas)\n",
    "            \n",
    "            print('Saving alphas...')\n",
    "            with open(path2 + 'data_integration_model_' + d + '_' + fusion_type + '_' + name + '_alphas.pkl', \"wb\") as f:\n",
    "                joblib.dump(splits_alphas, f)\n",
    "\n",
    "\n",
    "        #... [rest of the code is unchanged]\n",
    "\n",
    "\n",
    "        for df_name, df in data.items():\n",
    "            integration_probs = {\n",
    "            'RNAseq': [],\n",
    "            'microRNA': [],\n",
    "            }\n",
    "            integration_preds = []\n",
    "            for _, row in tqdm(df.iterrows()):\n",
    "                local_probs = {\n",
    "                    'RNAseq': [],\n",
    "                    'microRNA': [],\n",
    "                }\n",
    "                local_preds = []\n",
    "                for d_type in data_types:\n",
    "                    if row['Has ' + d_type] != -1:\n",
    "                        rnaseq = row[d_type+ 'Prob RNAseq']\n",
    "                        microRNA = row[d_type+ 'Prob microRNA']\n",
    "                        rnaseq_new = rnaseq / (rnaseq + microRNA)\n",
    "                        microRNA_new = microRNA / (rnaseq + microRNA)\n",
    "                        local_probs['RNAseq'].append(rnaseq_new)\n",
    "                        local_probs['microRNA'].append(microRNA_new)\n",
    "                        local_preds.append(row[d_type + ' Pred'])\n",
    "                    elif use_alphas:\n",
    "                        local_probs['RNAseq'].append(0)\n",
    "                        local_probs['microRNA'].append(0)\n",
    "\n",
    "                if fusion_type == 'probs':\n",
    "                    if use_alphas:\n",
    "                        if m_alphas:\n",
    "                        #alphas_manual = [0.65, 0.35]\n",
    "                            alphas_manual = m_alphas\n",
    "                            rnaseq_prob = integrate_probs(local_probs['RNAseq'], 0, alphas_manual)\n",
    "                            microRNA_prob = integrate_probs(local_probs['microRNA'], 1, alphas_manual)\n",
    "                        else:\n",
    "                            rnaseq_prob_new = integrate_probs(local_probs['RNAseq'], 0, splits_alphas[df_name])\n",
    "                            microRNA_prob_new = integrate_probs(local_probs['microRNA'], 1, splits_alphas[df_name])\n",
    "                            rnaseq_prob = rnaseq_prob / (rnaseq_prob + microRNA_prob_new)\n",
    "                            microRNA_prob = microRNA_prob / (rnaseq_prob + microRNA_prob)\n",
    "                    else:\n",
    "                        rnaseq_prob = integrate_probs(local_probs['RNAseq'])\n",
    "                        microRNA_prob = integrate_probs(local_probs['microRNA'])\n",
    "                    integration_probs['RNAseq'].append(rnaseq_prob)\n",
    "                    integration_probs['microRNA'].append(microRNA_prob)\n",
    "\n",
    "                    pred = np.argmax([rnaseq_prob,microRNA_prob], axis=0)\n",
    "                    integration_preds.append(pred)\n",
    "\n",
    "                elif fusion_type == 'preds':\n",
    "                    if len(local_preds) == 2:\n",
    "                        # if there are only two predictions, we need to fuse the probabilities\n",
    "                        rnaseq_prob = integrate_probs(local_probs['RNAseq'])\n",
    "                        microRNA_prob = integrate_probs(local_probs['microRNA'])\n",
    "\n",
    "                        integration_probs['RNAseq'].append(rnaseq_prob)\n",
    "                        integration_probs['microRNA'].append(microRNA_prob)\n",
    "\n",
    "                        pred = np.argmax([rnaseq_prob,microRNA_prob], axis=0)\n",
    "\n",
    "                    else:\n",
    "                        pred = integrate_preds(local_preds)\n",
    "                        if pred == 0:\n",
    "                            rnaseq_prob = 1\n",
    "                            microRNA_prob = 0\n",
    "                        elif pred == 1:\n",
    "                            rnaseq_prob = 0\n",
    "                            hlt_prob = 1\n",
    "                            microRNA_prob = 0\n",
    "                        else:\n",
    "                            rnaseq_prob = 0\n",
    "                            microRNA_prob = 0\n",
    "\n",
    "                        integration_probs['RNAseq'].append(rnaseq_prob)\n",
    "                        integration_probs['microRNA'].append(rnaseq_prob)\n",
    "\n",
    "\n",
    "                    integration_preds.append(pred)\n",
    "\n",
    "\n",
    "            for cls in integration_probs.keys():\n",
    "                df['Integration Prob '+ cls] = integration_probs[cls]\n",
    "\n",
    "            df['Integration Pred'] = integration_preds\n",
    "\n",
    "            # save to sheet\n",
    "            output_file_path = path + 'data_integration_model_' + d + '_' + fusion_type + '_' + name + '_split_' + str(df_name) + '.csv'\n",
    "            df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "1el1xcHbOZ_L",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting alphas from training set...\n",
      "X_train Length:  462\n",
      "y_train Length:  462\n",
      "X_train Length:  172\n",
      "y_train Length:  172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c395d798a084fca8889f8cf5dc38788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'Has rnaseq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/brain_span_data/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brain_span_data/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brain_span_data/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Has rnaseq'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m integration_model(data_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrnaseq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicroRNA\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      2\u001b[0m                   datasets\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSGD-all_sources\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   path1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m, path2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline_model_outputs/\u001b[39m\u001b[38;5;124m'\u001b[39m, fusion_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m'\u001b[39m, use_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[48], line 42\u001b[0m, in \u001b[0;36mintegration_model\u001b[0;34m(data_types, datasets, name, path1, path2, fusion_type, use_alphas, m_alphas)\u001b[0m\n\u001b[1;32m     40\u001b[0m splits_alphas \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dt, df \u001b[38;5;129;01min\u001b[39;00m data_train\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 42\u001b[0m     alphas \u001b[38;5;241m=\u001b[39m get_alphas_SGD(df, [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m], data_types)\n\u001b[1;32m     43\u001b[0m     splits_alphas[dt] \u001b[38;5;241m=\u001b[39m alphas\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(alphas)\n",
      "Cell \u001b[0;32mIn[3], line 161\u001b[0m, in \u001b[0;36mget_alphas_SGD\u001b[0;34m(df, classes, data_types)\u001b[0m\n\u001b[1;32m    159\u001b[0m x \u001b[38;5;241m=\u001b[39m [[],[],[]]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHas \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m d_type] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    162\u001b[0m         luad_new \u001b[38;5;241m=\u001b[39m row[d_type\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Prob LUAD\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    163\u001b[0m         hlt_new \u001b[38;5;241m=\u001b[39m row[d_type\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Prob HLT\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brain_span_data/lib/python3.11/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brain_span_data/lib/python3.11/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/brain_span_data/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Has rnaseq'"
     ]
    }
   ],
   "source": [
    "integration_model(data_types=[\"rnaseq\", \"microRNA\"],\n",
    "                  datasets=['test', 'train'], name=\"SGD-all_sources\",\n",
    "                  path1='data/', path2='baseline_model_outputs/', fusion_type='probs', use_alphas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x04\\x95\\xcf\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8c\\x13joblib.'\n"
     ]
    }
   ],
   "source": [
    "with open('baseline_model_outputs/rnaseq/X_train.pkl', 'rb') as f:\n",
    "    print(f.read(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqwp6cXbOeJK"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "data_model = pd.read_excel('../result_files/data_integration_model_test_probs_SGD-all_sources.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "accs = {\n",
    "    'RNAseq': [],\n",
    "    'miRNA': [],\n",
    "    'Integration': {'RNAseq': [], 'miRNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "f1_scores = {\n",
    "    'RNAseq': [],\n",
    "    'miRNA': [],\n",
    "    'Integration': {'RNAseq': [], 'miRNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "aucs = {\n",
    "    'RNAseq': [],\n",
    "    'microRNA': [],\n",
    "    'Integration': {'RNAseq': [], 'miRNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "auprcs = {\n",
    "    'RNAseq': [],\n",
    "    'microRNA': [],\n",
    "    'Integration': {'RNAseq': [], 'miRNA': [], 'Integration': []}\n",
    "}\n",
    "\n",
    "for d_type in ['RNAseq', 'miRNA']:\n",
    "        for df_name, df in data_model.items():\n",
    "            if d_type != 'Integration':\n",
    "                df_only = df.loc[df['Has '+ d_type] != -1]\n",
    "            else:\n",
    "                df_only = df\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only[d_type + ' Prob RNAseq'], df_only[d_type + ' Prob microRNA'])]\n",
    "            preds = df_only[d_type + ' Pred'].values\n",
    "            probs = np.asarray(probs)\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            accs[d_type].append(acc)\n",
    "            f1_scores[d_type].append(f1)\n",
    "            aucs[d_type].append(auc)\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "            auprcs[d_type].append(np.mean([aucpr1, aucpr2, aucpr3]))\n",
    "\n",
    "            # integration\n",
    "            probs_int = [[x,y,z] for x,y,z in zip(df_only['Integration Prob RNAseq'], df_only['Integration Prob microRNA'])]\n",
    "            preds_int = df_only['Integration Pred'].values\n",
    "            probs_int = np.asarray(probs_int)\n",
    "            real = df_only['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc_int = accuracy_score(real, preds_int)*100\n",
    "            f1_int = f1_score(real, preds_int, average='weighted')*100\n",
    "            auc_int = roc_auc_score(real, probs_int, multi_class='ovr')\n",
    "            accs['Integration'][d_type].append(acc_int)\n",
    "            f1_scores['Integration'][d_type].append(f1_int)\n",
    "            aucs['Integration'][d_type].append(auc_int)\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs_int[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs_int[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs_int[:, 2])\n",
    "            auprcs['Integration'][d_type].append(np.mean([aucpr1, aucpr2, aucpr3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTH6T9BkOjWc"
   },
   "outputs": [],
   "source": [
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs[d_type]),np.std(accs[d_type])))\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs['Integration'][d_type]),np.std(accs['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores[d_type]),np.std(f1_scores[d_type])))\n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores['Integration'][d_type]),np.std(f1_scores['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs[d_type]),np.std(aucs[d_type])))\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs['Integration'][d_type]),np.std(aucs['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs[d_type]),np.std(auprcs[d_type])))\n",
    "    print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs['Integration'][d_type]),np.std(auprcs['Integration'][d_type])))\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TztfMPZOlqP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accs = []\n",
    "f1_scores = []\n",
    "aucs = []\n",
    "auprcs = []\n",
    "data_model = pd.read_excel('../result_files/data_integration_model_test_probs_SGD-all_sources.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "d_type = 'Integration'\n",
    "for df_name, df in data_model.items():\n",
    "\n",
    "    df_only = df\n",
    "    probs = [[x,y,z] for x,y,z in zip(df_only[d_type + ' Prob RNAseq'], df_only[d_type + ' Prob microRNA'])]\n",
    "    probs = np.asarray(probs)\n",
    "    preds = df_only[d_type + ' Pred'].values\n",
    "\n",
    "    real = df_only['Real'].values\n",
    "    real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "\n",
    "    aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "    aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "    aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "\n",
    "    acc = accuracy_score(real, preds)*100\n",
    "    f1 = f1_score(real, preds, average='weighted')*100\n",
    "    auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "    accs.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    aucs.append(auc)\n",
    "    auprcs.append(np.mean([aucpr1, aucpr2, aucpr3]))\n",
    "\n",
    "print(d_type + ' ACC: {}+-{}'.format(np.mean(accs),np.std(accs)))\n",
    "print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores),np.std(f1_scores)))\n",
    "print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs),np.std(aucs)))\n",
    "print(d_type + ' AUPRC: {}+-{}'.format(np.mean(auprcs),np.std(auprcs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWXc0-UkOrIZ"
   },
   "outputs": [],
   "source": [
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs[d_type]),np.std(accs[d_type])))\n",
    "    print(d_type + ' ACC: {}+-{}'.format(np.mean(accs['Integration'][d_type]),np.std(accs['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores[d_type]),np.std(f1_scores[d_type])))\n",
    "    print(d_type + ' F1: {}+-{}'.format(np.mean(f1_scores['Integration'][d_type]),np.std(f1_scores['Integration'][d_type])))\n",
    "    print(5*'-')\n",
    "print(10*'-')\n",
    "\n",
    "for d_type in ['RNAseq', 'microRNA']:\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs[d_type]),np.std(aucs[d_type])))\n",
    "    print(d_type + ' AUC: {}+-{}'.format(np.mean(aucs['Integration'][d_type]),np.std(aucs['Integration'][d_type])))\n",
    "    print(5*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfeTXJfOO0Nu"
   },
   "outputs": [],
   "source": [
    "os.mkdir('results_SGD/two-sources-integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssniqygiO02Z"
   },
   "outputs": [],
   "source": [
    "data_types = ['RNAseq', 'microRNA']\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        integration_model(data_types=[d_type1, d_type2],\n",
    "                          datasets=['test'], name=name,\n",
    "                          path='results_SGD/two-sources-integration/',\n",
    "                          fusion_type='probs', use_alphas=True)\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1p6XaIX7O5Qg"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "data_types = ['RNAseq', 'microRNA']\n",
    "\n",
    "accs = {}\n",
    "f1_scores = {}\n",
    "aucs = {}\n",
    "aucprcs = {}\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        data_model = pd.read_excel('../result_files/two-sources-integration/data_integration_model_test_probs_'+name+'.xlsx',\n",
    "              sheet_name=[0,1,2,3,4,5,6,7,8,9],engine='openpyxl')\n",
    "\n",
    "        accs[name] = {}\n",
    "        f1_scores[name] = {}\n",
    "        aucs[name] = {}\n",
    "        aucprcs[name] = {}\n",
    "\n",
    "        accs[name]['Integration'] = []\n",
    "        accs[name][d_type1+'Int'] = []\n",
    "        accs[name][d_type2+'Int'] = []\n",
    "\n",
    "        f1_scores[name]['Integration'] = []\n",
    "        f1_scores[name][d_type1+'Int'] = []\n",
    "        f1_scores[name][d_type2+'Int'] = []\n",
    "\n",
    "        aucs[name]['Integration'] = []\n",
    "        aucs[name][d_type1+'Int'] = []\n",
    "        aucs[name][d_type2+'Int'] = []\n",
    "\n",
    "        aucprcs[name]['Integration'] = []\n",
    "        aucprcs[name][d_type1+'Int'] = []\n",
    "        aucprcs[name][d_type2+'Int'] = []\n",
    "\n",
    "        for df_name, df in data_model.items():\n",
    "            # take those where the two sources has data\n",
    "            df_only = df.loc[(df['Has '+ d_type1] != -1) | (df['Has ' + d_type2] != -1)]\n",
    "            df_dt1 = df.loc[df['Has '+ d_type1] != -1]\n",
    "            df_dt2 = df.loc[df['Has '+ d_type2] != -1]\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_only['Integration Prob RNAseq'], df_only['Integration Prob microRNA'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_only['Integration Pred'].values\n",
    "            real = df_only['Real'].values\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "\n",
    "            accs[name]['Integration'].append(acc)\n",
    "            f1_scores[name]['Integration'].append(f1)\n",
    "            aucs[name]['Integration'].append(auc)\n",
    "            aucprcs[name]['Integration'].append([aucpr1,aucpr2,aucpr3])\n",
    "\n",
    "            # dtype1\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_dt1['Integration Prob RNAseq'], df_dt1['Integration Prob microRNA'])]\n",
    "            probs = np.asarray(probs)\n",
    "            preds = df_dt1['Integration Pred'].values\n",
    "            real = df_dt1['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "\n",
    "            accs[name][d_type1+'Int'].append(acc)\n",
    "            f1_scores[name][d_type1+'Int'].append(f1)\n",
    "            aucs[name][d_type1+'Int'].append(auc)\n",
    "            aucprcs[name][d_type1+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "\n",
    "            # dtype2\n",
    "            probs = [[x,y,z] for x,y,z in zip(df_dt2['Integration Prob RNAseq'], df_dt1['Integration Prob microRNA'])]\n",
    "            preds = df_dt2['Integration Pred'].values\n",
    "            probs = np.asarray(probs)\n",
    "            real = df_dt2['Real'].values\n",
    "            real_binarized = label_binarize(real, classes=[*range(3)])\n",
    "            acc = accuracy_score(real, preds)*100\n",
    "            f1 = f1_score(real, preds, average='weighted')*100\n",
    "            auc = roc_auc_score(real, probs, multi_class='ovr')\n",
    "            aucpr1 = average_precision_score(real_binarized[:, 0], probs[:, 0])\n",
    "            aucpr2 = average_precision_score(real_binarized[:, 1], probs[:, 1])\n",
    "            aucpr3 = average_precision_score(real_binarized[:, 2], probs[:, 2])\n",
    "\n",
    "            accs[name][d_type2+'Int'].append(acc)\n",
    "            f1_scores[name][d_type2+'Int'].append(f1)\n",
    "            aucs[name][d_type2+'Int'].append(auc)\n",
    "            aucprcs[name][d_type2+'Int'].append(np.mean([aucpr1,aucpr2,aucpr3]))\n",
    "\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bScjwKnGO5_D"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' ACC: {}+-{}'.format(np.mean(accs[name]['Integration']),np.std(accs[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type1+'Int']),np.std(accs[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' ACC: {}+-{}'.format(np.mean(accs[name][d_type2+'Int']),np.std(accs[name][d_type2+'Int'])))\n",
    "\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "print(10*'-')\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(5*'-')\n",
    "        print(name + ' F1: {}+-{}'.format(np.mean(f1_scores[name]['Integration']),np.std(f1_scores[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type1+'Int']),np.std(f1_scores[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' F1: {}+-{}'.format(np.mean(f1_scores[name][d_type2+'Int']),np.std(f1_scores[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "print(10*'-')\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' AUC: {}+-{}'.format(np.mean(aucs[name]['Integration']),np.std(aucs[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type1+'Int']),np.std(aucs[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' AUC: {}+-{}'.format(np.mean(aucs[name][d_type2+'Int']),np.std(aucs[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1\n",
    "\n",
    "i = 0\n",
    "j = 1\n",
    "for d_type1 in data_types[i:]:\n",
    "    for d_type2 in data_types[j:]:\n",
    "        print(5*'-')\n",
    "        name = d_type1 + '-' + d_type2\n",
    "        print(name + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name]['Integration']),np.std(aucprcs[name]['Integration'])))\n",
    "        print(d_type1+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type1+'Int']),np.std(aucprcs[name][d_type1+'Int'])))\n",
    "        print(d_type2+'Int' + ' AUPRC: {}+-{}'.format(np.mean(aucprcs[name][d_type2+'Int']),np.std(aucprcs[name][d_type2+'Int'])))\n",
    "    i += 1\n",
    "    j = i + 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:brain_span_data]",
   "language": "python",
   "name": "conda-env-brain_span_data-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
